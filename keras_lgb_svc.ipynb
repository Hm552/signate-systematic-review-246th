{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "keras_lgb_svc",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1eQw1o8EcsybPVsLzpteEbo0Y_96MfvLz",
      "authorship_tag": "ABX9TyPYfMW4cXPMlt8azlZEpJre",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hm552/signate-systematic-review-246th/blob/main/keras_lgb_svc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKWFWMXL2WMH",
        "outputId": "4a5e4a57-b1ae-4a96-e50d-3f338dd72302"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI94Vom73UQz"
      },
      "source": [
        "# データセット保存先ディレクトリ\n",
        "if 'google.colab' in sys.modules:\n",
        "    dataset_dir=\"/content/drive/MyDrive/SRWS\"\n",
        "else:\n",
        "    dataset_dir=\"/path/to\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B9pHXpi9FHT4",
        "outputId": "f36fa76a-0fd9-4bf1-96f0-1a24dbc10151"
      },
      "source": [
        "! pip install --quiet transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 71.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W0j_JrG2FdkP",
        "outputId": "bee39fb6-0b9f-4adf-d6a8-a95128c2ea9e"
      },
      "source": [
        "! pip install --quiet tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwHYFtS8FX3r"
      },
      "source": [
        "import logging\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "#import transformers\n",
        "#from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U_OtTa9rFmey"
      },
      "source": [
        "class Logger:\n",
        "    \"\"\"log を残す用のクラス\"\"\"\n",
        "    def __init__(self, path):\n",
        "        self.general_logger = logging.getLogger(path)\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n",
        "        if len(self.general_logger.handlers) == 0:\n",
        "            self.general_logger.addHandler(stream_handler)\n",
        "            self.general_logger.addHandler(file_general_handler)\n",
        "            self.general_logger.setLevel(logging.INFO)\n",
        "\n",
        "    def info(self, message):\n",
        "        # display time\n",
        "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
        "\n",
        "    @staticmethod\n",
        "    def now_string():\n",
        "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TIuSJ7BF7o-"
      },
      "source": [
        "import os\n",
        "INPUT = os.path.join(dataset_dir, \"input\")\n",
        "OUTPUT = os.path.join(dataset_dir, \"output\")\n",
        "EXP = \"211002_1\"\n",
        "OUTPUT_EXP = os.path.join(OUTPUT, EXP)\n",
        "OUTPUT_EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n",
        "OUTPUT_EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n",
        "OUTPUT_EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n",
        "SUBMISSION = os.path.join(dataset_dir, \"submission\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "csweT4hdFsT5",
        "outputId": "26ac0ffe-3f7b-4ea4-a0d8-95b43734c9aa"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")  # ignore warnings\n",
        "logger = Logger(OUTPUT_EXP)  # set logger \n",
        "\n",
        "# TPU setting\n",
        "try:\n",
        "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    TPU = None\n",
        "    print('INFO: Not connected to a TPU runtime')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.27.242.2:8470']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFAtDNnNGpmy"
      },
      "source": [
        "def build_auto_model():\n",
        "    \"\"\"TFAutoModel\"\"\"\n",
        "    transformer = TFAutoModel.from_pretrained(Config.model)\n",
        "    input_word_ids = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    x = transformer(input_word_ids, attention_mask=attention_mask)\n",
        "    #x = x[0][:, 0, :]  # cls\n",
        "\n",
        "    last_hidden_state = x[0]\n",
        "    last_hidden_state = tf.keras.layers.Dropout(0.1)(last_hidden_state)\n",
        "    x_avg = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n",
        "    x_max = tf.keras.layers.GlobalMaxPooling1D()(last_hidden_state)\n",
        "    x = tf.keras.layers.Concatenate()([x_avg, x_max])\n",
        "\n",
        "    samples = []    \n",
        "    for n in range(8):\n",
        "        sample_mask = tf.keras.layers.Dense(64, activation='relu', name = f'dense_{n}')\n",
        "        sample = tf.keras.layers.Dropout(.5)(x)\n",
        "        sample = sample_mask(sample)\n",
        "        sample = tf.keras.layers.Dense(1, activation='sigmoid', name=f'sample_{n}')(sample)\n",
        "        samples.append(sample)\n",
        "      \n",
        "    output = tf.keras.layers.Average(name='output')(samples)\n",
        "\n",
        "    #output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, attention_mask],\n",
        "                                  outputs=[output])\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(lr=Config.lr, weight_decay=Config.weight_decay)\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss = [tf.keras.losses.BinaryCrossentropy()],  \n",
        "                  metrics = [\"accuracy\"]) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp0AwGCuHNxO"
      },
      "source": [
        "class Config:\n",
        "    name_v1 = \"tf-roberta-base\"\n",
        "    model = \"roberta-base\"\n",
        "    max_length = 256\n",
        "    lr = 0.00002\n",
        "    weight_decay = 1e-5\n",
        "    n_fold = 4 \n",
        "    epochs = 32\n",
        "    train_batch_size = 64\n",
        "    steps_per_epochs = 30\n",
        "    seeds = [2021]\n",
        "    target_col = \"judgement\"\n",
        "    text_col = \"text\"  # title + abst\n",
        "    valid_batch_size = 64\n",
        "    test_batch_size = 64\n",
        "    class_weight = None\n",
        "    submit = True\n",
        "    debug = False\n",
        "\n",
        "if Config.debug:\n",
        "    Config.epochs = 1\n",
        "    Config.n_fold = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jeODoCXlHSME"
      },
      "source": [
        "def opt_fbeta_threshold(y_true, y_pred):\n",
        "    \"\"\"fbeta score計算時のthresholdを最適化\"\"\"\n",
        "    def opt_(x): \n",
        "        return -fbeta_score(y_true, y_pred >= x, beta=7)\n",
        "    result = minimize(opt_, x0=np.array([0.1]), method='Powell')\n",
        "    best_threshold = result['x'].item()\n",
        "    return best_threshold\n",
        "\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    \"\"\"fbeta(beta=7)の閾値最適化評価関数\"\"\"\n",
        "    bt = opt_fbeta_threshold(y_true, y_pred)\n",
        "    print(f\"bt:{bt}\")\n",
        "    score = fbeta_score(y_true, y_pred >= bt, beta=7)\n",
        "    return score\n",
        "\n",
        "\n",
        "def skf(train, n_splits, random_state):\n",
        "    \"\"\"層化KFoldのインデックスのリストを作成\"\"\"\n",
        "    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "    return list(skf.split(train, train[Config.target_col]))\n",
        "\n",
        "\n",
        "def encode_texts(texts, tokenizer, max_length):\n",
        "    \"\"\"text encoding\"\"\"\n",
        "    encoded_dict = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        max_length = max_length,\n",
        "    )\n",
        "    return dict(encoded_dict)\n",
        "\n",
        "def get_dataset(X, y=None, dataset=\"test\"):\n",
        "    \"\"\"データをtf.Datasetの形式に変更\"\"\"\n",
        "    if dataset==\"train\":\n",
        "        train_dataset = (\n",
        "            tf.data.Dataset\n",
        "            .from_tensor_slices((X, y))\n",
        "            .shuffle(2048)\n",
        "            .batch(Config.train_batch_size)\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "            )\n",
        "        if Config.steps_per_epochs is not None:\n",
        "            train_dataset = train_dataset.repeat()\n",
        "\n",
        "        return train_dataset\n",
        "\n",
        "    elif dataset==\"valid\":\n",
        "        valid_dataset = (\n",
        "            tf.data.Dataset\n",
        "            .from_tensor_slices((X, y))\n",
        "            .batch(Config.valid_batch_size)\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        )\n",
        "        return valid_dataset\n",
        "    \n",
        "    elif dataset==\"test\":\n",
        "        test_dataset = (\n",
        "            tf.data.Dataset\n",
        "            .from_tensor_slices(X)\n",
        "            .batch(Config.test_batch_size)\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        )\n",
        "        return test_dataset\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer():\n",
        "    \"\"\"model&tokenizer を取得 (TPU ok)\"\"\"\n",
        "    if TPU:\n",
        "        tf.config.experimental_connect_to_cluster(TPU)\n",
        "        tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "        tpu_strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_auto_model()\n",
        "    else:\n",
        "        model = build_auto_model()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Config.model)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_class_weight(target, weight):\n",
        "    \"\"\"class weights の作成\"\"\"\n",
        "    if weight == \"balanced\":\n",
        "        class_weights = class_weight.compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(target),\n",
        "            y=target)\n",
        "        class_weights = dict(enumerate(class_weights))\n",
        "    elif weight is None:\n",
        "        class_weights = None\n",
        "    else:\n",
        "        # ex) weight = {0:0.2, 1:0.98}\n",
        "        class_weights = weight\n",
        "    \n",
        "    return class_weights\n",
        "\n",
        "\n",
        "def training_v1(train_df, valid_df, model, tokenizer, filepath):\n",
        "    \"\"\"training 用の関数\"\"\"\n",
        "    tr_text = encode_texts(texts=train_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    va_text = encode_texts(texts=valid_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    tr_dataset = get_dataset(X=tr_text, y=train_df[Config.target_col].values, dataset=\"train\")\n",
        "    va_dataset = get_dataset(X=va_text, y=valid_df[Config.target_col].values, dataset=\"valid\")\n",
        "\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath, \n",
        "        moniter=\"val_loss\", \n",
        "        verbose=1, \n",
        "        save_best_only=True, \n",
        "        save_weights_only=True,\n",
        "        mode=\"min\")\n",
        "    \n",
        "    model.fit(tr_dataset, \n",
        "              epochs=Config.epochs, \n",
        "              verbose=1, \n",
        "              callbacks=[checkpoint],\n",
        "              validation_data=va_dataset, \n",
        "              steps_per_epoch=Config.steps_per_epochs,\n",
        "              class_weight=get_class_weight(train_df[Config.target_col], weight=Config.class_weight))\n",
        "\n",
        "\n",
        "def inference_v1(test_df, model, tokenizer, filepath):\n",
        "    \"\"\"推論用の関数\"\"\"\n",
        "    model.load_weights(filepath)\n",
        "    te_text = encode_texts(texts=test_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    te_dataset = get_dataset(X=te_text, y=None, dataset=\"test\")\n",
        "    preds = model.predict(te_dataset)\n",
        "\n",
        "\n",
        "    return preds.reshape(-1)\n",
        "\n",
        "'''def greed_encode(data, max_len) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "  \n",
        "    for i in range(len(data.text)):\n",
        "        \n",
        "        encoded = tokenizer.encode_plus(data.text[i], add_special_tokens=True, max_length=max_len, pad_to_max_length=True)\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids),np.array(attention_masks)'''\n",
        "\n",
        "def features_from_bertlayer(train_df, test_df, model, tokenizer, filepath, name):\n",
        "    \"\"\"推論用の関数\"\"\"\n",
        "    #model.load_weights(filepath)\n",
        "    cls_layer_model = tf.keras.models.Model(model.input, outputs=model.get_layer(f'dense_1').output)\n",
        "\n",
        "    tr_text = encode_texts(texts=train_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    #tr_dataset = get_dataset(X=tr_text, y=train_df[Config.target_col].values, dataset=\"train\")\n",
        "    te_text = encode_texts(texts=test_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    #te_dataset = get_dataset(X=te_text, y=None, dataset=\"test\")\n",
        "\n",
        "    X_train = cls_layer_model.predict([np.array(tr_text['input_ids']), np.array(tr_text['attention_mask'])])\n",
        "    X_test = cls_layer_model.predict([np.array(te_text['input_ids']), np.array(te_text['attention_mask'])])\n",
        "    y_train = train_df[Config.target_col].values\n",
        "\n",
        "    return X_train, X_test, y_train\n",
        "\n",
        "def lgb_cv(X_train, X_test, y_train, name):\n",
        "    N_FOLDS = 3\n",
        "    print(f'LGBM')\n",
        "    folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X_train))\n",
        "    sub = np.zeros(len(X_test))\n",
        "    params = {'boosting_type': 'dart'}\n",
        "    for fold_, (train_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
        "        X_train_cv, y_train_cv = pd.DataFrame(X_train).loc[train_idx], pd.DataFrame(y_train).loc[train_idx]\n",
        "        X_val, y_val = pd.DataFrame(X_train).loc[val_idx], pd.DataFrame(y_train).loc[val_idx]\n",
        "        train_data = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
        "        val_data = lgb.Dataset(X_val, label=y_val)\n",
        "        watchlist = [train_data, val_data]\n",
        "        clf = lgb.train(params, train_set = train_data, valid_sets=watchlist)\n",
        "        oof[val_idx] = clf.predict(X_val)\n",
        "        sub += clf.predict(X_test)/folds.n_splits\n",
        "    oof_all_lgb = oof\n",
        "    sub_all_lgb = sub\n",
        "    #print(accuracy_score(y_train, np.round(oof).astype(int)),'\\n')\n",
        "    score = metrics(y_train, oof_all_lgb)\n",
        "    logger.info(f\"{name}_lgb >>> val score:{score:.4f}\")\n",
        "    return oof_all_lgb, sub_all_lgb\n",
        "\n",
        "def svc_cv(X_train, X_test, y_train, name):\n",
        "    N_FOLDS = 3\n",
        "    print(f'SVC')\n",
        "    folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X_train))\n",
        "    sub = np.zeros(len(X_test))\n",
        "    scores = [0 for _ in range(folds.n_splits)]\n",
        "    for fold_, (train_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
        "        X_train_cv, y_train_cv = pd.DataFrame(X_train).loc[train_idx], pd.DataFrame(y_train).loc[train_idx]\n",
        "        X_val, y_val = pd.DataFrame(X_train).loc[val_idx], pd.DataFrame(y_train).loc[val_idx]\n",
        "        clf = SVC(kernel='rbf', C=1.75, gamma = 0.1, probability = True).fit(X_train_cv, y_train_cv)\n",
        "        oof[val_idx] = clf.predict_proba(X_val)[:,1]\n",
        "        sub += clf.predict_proba(X_test)[:,1]/folds.n_splits\n",
        "    oof_all_svc = oof\n",
        "    sub_all_svc = sub    \n",
        "    #print(accuracy_score(y_train, np.round(oof).astype(int)),'\\n')\n",
        "    score = metrics(y_train, oof_all_svc)\n",
        "    logger.info(f\"{name}_svc >>> val score:{score:.4f}\")\n",
        "    return oof_all_svc, sub_all_svc\n",
        "\n",
        "\n",
        "def train_cv_v1(train, cv, metrics, name, dir):\n",
        "    \"\"\"cross validationの実行関数 (train)\"\"\"\n",
        "\n",
        "    oof = np.zeros(len(train))\n",
        "    for i_fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "        K.clear_session()\n",
        "        model, tokenizer = get_model_and_tokenizer()\n",
        "\n",
        "        tr_df, va_df = train.iloc[tr_idx].reset_index(), train.iloc[va_idx].reset_index()\n",
        "        filepath = os.path.join(dir, f\"{name}_fold{i_fold+1}.h5\")\n",
        "\n",
        "        if not os.path.isfile(filepath):  # 学習済みモデルがあればtrainingしない\n",
        "            training_v1(tr_df, va_df, model, tokenizer, filepath)\n",
        "        \n",
        "        preds = inference_v1(va_df, model, tokenizer, filepath)\n",
        "\n",
        "        score = metrics(np.array(va_df[Config.target_col]), preds)\n",
        "        logger.info(f\"{name}_fold{i_fold+1} >>> val socre:{score:.4f}\")\n",
        "        oof[va_idx] = preds\n",
        "    \n",
        "    score = metrics(np.array(train[Config.target_col]), oof)\n",
        "    logger.info(f\"{name} >>> val score:{score:.4f}\")\n",
        "    return oof\n",
        "        \n",
        "def predict_cv_lgb_svc(train, test, name, dir):\n",
        "    \"\"\"cross validationの実行関数 (test)\"\"\"\n",
        "    lgb_preds_fold = []\n",
        "    svc_preds_fold = []\n",
        "    lgb_oof_fold = []\n",
        "    svc_oof_fold = []\n",
        "    for i_fold in range(Config.n_fold):\n",
        "        model, tokenizer = get_model_and_tokenizer()\n",
        "        filepath = os.path.join(dir, f\"{name}_fold{i_fold+1}.h5\")\n",
        "        # grab last layer and use LGBM and SVC\n",
        "        X_train, X_test, y_train = features_from_bertlayer(train, test, model, tokenizer, filepath, name)\n",
        "        lgb_oof, lgb_preds = lgb_cv(X_train, X_test, y_train, name)\n",
        "        svc_oof, svc_preds = svc_cv(X_train, X_test, y_train, name)\n",
        "\n",
        "        lgb_preds_fold.append(lgb_preds)\n",
        "        svc_preds_fold.append(svc_preds)\n",
        "        lgb_oof_fold.append(lgb_oof)\n",
        "        svc_oof_fold.append(svc_oof)\n",
        "\n",
        "        logger.info(f\"{name}_fold{i_fold+1} inference\")\n",
        "    \n",
        "    lgb_pred = np.mean(lgb_preds_fold, axis=0)\n",
        "    svc_pred = np.mean(svc_preds_fold, axis=0)\n",
        "    lgb_oof_ = np.mean(lgb_oof_fold, axis=0)\n",
        "    svc_oof_ = np.mean(svc_oof_fold, axis=0)\n",
        "    return lgb_pred, svc_pred, lgb_oof_, svc_oof_\n",
        "\n",
        "def predict_cv_v1(train, test, name, dir):\n",
        "    \"\"\"cross validationの実行関数 (test)\"\"\"\n",
        "    preds_fold = []\n",
        "    for i_fold in range(Config.n_fold):\n",
        "        model, tokenizer = get_model_and_tokenizer()\n",
        "        filepath = os.path.join(dir, f\"{name}_fold{i_fold+1}.h5\")\n",
        "        preds = inference_v1(test, model, tokenizer, filepath)\n",
        "        preds_fold.append(preds)\n",
        "\n",
        "        logger.info(f\"{name}_fold{i_fold+1} inference\")\n",
        "    \n",
        "    pred = np.mean(preds_fold, axis=0)\n",
        "    return pred\n",
        "\n",
        "def visualize_confusion_matrix(\n",
        "        y_true,\n",
        "        pred_label,\n",
        "        height=.6,\n",
        "        labels=None):\n",
        "    \"\"\"混合行列をプロット \n",
        "    (https://www.guruguru.science/competitions/11/discussions/2fb11851-67d0-4e96-a4b1-5629b944f363/)\"\"\"\n",
        "    \n",
        "    conf = confusion_matrix(y_true=y_true,\n",
        "                            y_pred=pred_label,\n",
        "                            normalize='true')\n",
        "\n",
        "    n_labels = len(conf)\n",
        "    size = n_labels * height\n",
        "    fig, ax = plt.subplots(figsize=(size * 4, size * 3))\n",
        "    sns.heatmap(conf, cmap='Blues', ax=ax, annot=True, fmt='.2f')\n",
        "    ax.set_ylabel('Label')\n",
        "    ax.set_xlabel('Predict')\n",
        "\n",
        "    if labels is not None:\n",
        "        ax.set_yticklabels(labels)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.tick_params('y', labelrotation=0)\n",
        "        ax.tick_params('x', labelrotation=90)\n",
        "\n",
        "    plt.show()\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2HWKPH02qOcn"
      },
      "source": [
        "def main():\n",
        "    # load data\n",
        "    train = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
        "    test = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n",
        "\n",
        "    if Config.debug:\n",
        "        train = train.sample(1000, random_state=Config.seeds[0]).reset_index(drop=True)\n",
        "        test = test.sample(1000, random_state=Config.seeds[0]).reset_index(drop=True)\n",
        "\n",
        "    # preprocess\n",
        "    train[\"text\"] = train[\"title\"] + \" \" + train[\"abstract\"].fillna(\"\")\n",
        "    test[\"text\"] = test[\"title\"] + \" \" + test[\"abstract\"].fillna(\"\")\n",
        "\n",
        "    # training\n",
        "    oof_df = pd.DataFrame()\n",
        "    for seed in Config.seeds:\n",
        "        name = f\"{Config.name_v1}-{seed}\"\n",
        "        oof = train_cv_v1(train, \n",
        "                        cv=skf(train, n_splits=Config.n_fold, random_state=seed),\n",
        "                        metrics=metrics, \n",
        "                        name=name, \n",
        "                        dir=OUTPUT_EXP_MODEL)\n",
        "        oof_df[name] = oof\n",
        "    oof_df.to_csv(os.path.join(OUTPUT_EXP_PREDS, \"tf_oof.csv\"), index=False)  # oof 予測値(prob)を保存\n",
        "\n",
        "    #oof_df = pd.read_csv(os.path.join(OUTPUT_EXP_PREDS, \"tf_oof.csv\"))  # oof 予測値(prob)を保存\n",
        "\n",
        "    # inference\n",
        "    preds_df = pd.DataFrame()\n",
        "    for seed in Config.seeds:\n",
        "        name = f\"{Config.name_v1}-{seed}\"\n",
        "        pred = predict_cv_v1(train, test, name, dir=OUTPUT_EXP_MODEL)\n",
        "        preds_df[name] = pred\n",
        "    \n",
        "    preds_df.to_csv(os.path.join(OUTPUT_EXP_PREDS, \"tf_preds.csv\"), index=False)  # test予測値(prob)を保存\n",
        "    #preds_df = pd.read_csv(os.path.join(OUTPUT_EXP_PREDS, \"tf_oof.csv\"))  # oof 予測値(prob)を保存\n",
        "    \n",
        "    # inference\n",
        "    for seed in Config.seeds:\n",
        "        name = f\"{Config.name_v1}-{seed}\"\n",
        "        lgb_pred, svc_pred, lgb_oof_, svc_oof_ = predict_cv_lgb_svc(train, test, name, dir=OUTPUT_EXP_MODEL)\n",
        "        preds_df[name + \"_lgb\"] = lgb_pred\n",
        "        preds_df[name + \"_svc\"] = svc_pred\n",
        "        oof_df[name + \"_lgb\"] = lgb_oof_\n",
        "        oof_df[name + \"_svc\"] = svc_oof_\n",
        "    \n",
        "    oof_df.to_csv(os.path.join(OUTPUT_EXP_PREDS, \"oof.csv\"), index=False)  # oof 予測値(prob)\n",
        "    preds_df.to_csv(os.path.join(OUTPUT_EXP_PREDS, \"preds.csv\"), index=False)  # test予測値(prob)を保存\n",
        "\n",
        "    y_true = train[Config.target_col]\n",
        "    y_pred = oof_df.mean(axis=1)\n",
        "\n",
        "    best_threshold = opt_fbeta_threshold(y_true.values, y_pred.values)\n",
        "    oof_score = fbeta_score(y_true, y_pred >= best_threshold, beta=7)\n",
        "    comments = f\"score:{oof_score:.4f}/threshold:{best_threshold}\"  # 最終的なスコアと閾値 (スペースがあるとsubmit時のコメントエラーが出る)\n",
        "    logger.info(comments)\n",
        "\n",
        "    fig = visualize_confusion_matrix(y_true, y_pred>=best_threshold)  # 混合行列を表示&save\n",
        "    fig.savefig(os.path.join(OUTPUT_EXP_FIG, \"cm.png\"), dpi=300)\n",
        "\n",
        "    # submit\n",
        "    if Config.submit:\n",
        "        sub_df = pd.read_csv(os.path.join(INPUT, \"sample_submit.csv\"), header=None)\n",
        "        sub_df.columns = [\"id\", \"judgement\"]\n",
        "\n",
        "        preds = preds_df.mean(axis=1)\n",
        "        sub_df[\"judgement\"] = (preds.values >= best_threshold) * 1\n",
        "        \n",
        "        filepath = os.path.join(SUBMISSION, f\"{EXP}.csv\")\n",
        "        sub_df.to_csv(filepath, index=False, header=False)\n",
        "        \n",
        "        # signate api を使ってsubmit\n",
        "        #! signate submit --competition-id=471 $filepath --note $comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyn8sRI_VBml"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}